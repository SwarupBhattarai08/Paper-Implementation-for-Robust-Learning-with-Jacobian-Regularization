{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jacobian Reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aviral09/JacobianReg/blob/master/Jacobian_Reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4ISvyR-fIGP"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from torch import nn, norm, randn, ones, zeros, autograd, addcdiv\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from torchsummary import summary\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GzclPHOxYaCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading datasets\n",
        "cifar_train = datasets.CIFAR10(\n",
        "    root=\"data/CIFAR\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "cifar_test = datasets.CIFAR10(\n",
        "    root=\"data/CIFAR\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tO1r0Glfi4q",
        "outputId": "77c0a75d-7cdf-4add-ba50-d417cef50a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "cifar_train_dataloader = DataLoader(cifar_train, batch_size=batch_size)\n",
        "cifar_test_dataloader = DataLoader(cifar_test, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "4ttoXoiFfkKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} for training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hix31Szofl_5",
        "outputId": "83bb97b9-9ea1-43ae-bc92-5c21db59fdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda for training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Jreg(nn.Module):\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "    super(Jreg, self).__init__()\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    nproj = self.n\n",
        "    jreg=0\n",
        "    for i in range(nproj):\n",
        "      if nproj == y.shape[1]:\n",
        "        v = zeros(y.shape[0], y.shape[1])\n",
        "        v[:,i] = 1\n",
        "      elif y.shape[1] == 1:\n",
        "        v = ones(y.shape[0])\n",
        "      else:\n",
        "        v = randn(y.shape[0], y.shape[1])\n",
        "        vnorm = norm(v, 2, 1, True)\n",
        "        v = addcdiv(zeros(y.shape[0], y.shape[1]), 1.0, v, vnorm)\n",
        "      v= v.to(device)\n",
        "      j, = autograd.grad(y.reshape(-1), x, v.reshape(-1), retain_graph=True, create_graph=True)\n",
        "      jreg += (y.shape[1])*(norm(j, dim=None)**2) /(nproj*y.shape[0])\n",
        "    return 0.5*jreg\n"
      ],
      "metadata": {
        "id": "BwuOKWi0w5lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFARModel,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 3)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "    self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "n1kd0yNSfnT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifarmodel = CIFARModel().to(device)\n",
        "summary(cifarmodel, (3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD4tNia5f0Bc",
        "outputId": "69d97693-800c-4b19-d787-60645e41ddfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 30, 30]             168\n",
            "         MaxPool2d-2            [-1, 6, 15, 15]               0\n",
            "            Conv2d-3           [-1, 16, 13, 13]             880\n",
            "         MaxPool2d-4             [-1, 16, 6, 6]               0\n",
            "            Linear-5                  [-1, 120]          69,240\n",
            "            Linear-6                   [-1, 84]          10,164\n",
            "            Linear-7                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 81,302\n",
            "Trainable params: 81,302\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.08\n",
            "Params size (MB): 0.31\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNEMCwIcGboM"
      },
      "source": [
        "def train(dataloader,model,loss_fn,optimizer, reg):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    x = torch.tensor(x, requires_grad=True)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Calculating model predictions\n",
        "    y_hat = model(x)\n",
        "    mainloss = loss_fn(y_hat, y)\n",
        "    jloss = reg(x,y_hat)\n",
        "    loss = mainloss + jlambda*jloss\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        loss, current, mainloss, jloss = loss.item(), batch, mainloss.item(), jloss.item()        \n",
        "        print(f\"Training loss Total loss: {loss} Main loss: {mainloss} Jacob Loss: {jloss} [{current}/{int(size/batch_size)+1}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQJAsNIVDd18"
      },
      "source": [
        "def test(dataloader, model, loss_fn, jlambda):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct, mainloss, jloss = 0, 0, 0, 0\n",
        "    flag = False\n",
        "    reg = 0\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        X.requires_grad = True\n",
        "        pred = model(X)\n",
        "        if flag == False:\n",
        "          reg = Jreg(pred.shape[1])\n",
        "          flag = True\n",
        "        mainloss += loss_fn(pred, y).item()\n",
        "        jloss += reg(X, pred)\n",
        "        test_loss += mainloss + jloss*jlambda\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    mainloss /= num_batches\n",
        "    jloss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct)}%, Avg loss: {test_loss}, Main loss: {mainloss}, Jacob loss: {jloss} \\n\")\n",
        "    return test_loss, correct, mainloss, jloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss,correct, mainloss, jloss = 0, 0, 0, 0\n",
        "jlambda = 0.1\n",
        "nproj = 2\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cifarmodel.parameters(),lr=1e-4)\n",
        "reg = Jreg(nproj)\n",
        "epochs = 20\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(cifar_train_dataloader, cifarmodel, loss_fn, optimizer, reg)\n",
        "    gc.collect()\n",
        "print(\"Done!\")\n",
        "\n",
        "loss, correct, mainloss, jloss = test(cifar_test_dataloader, cifarmodel, loss_fn, jlambda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2x6n4dKf0Yz",
        "outputId": "a7088c80-084c-48de-ef70-41da85503bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loss Total loss: 2.307790994644165 Main loss: 2.307739734649658 Jacob Loss: 0.0005135788815096021 [0/782]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss Total loss: 2.2897164821624756 Main loss: 2.289475440979004 Jacob Loss: 0.002411404624581337 [100/782]\n",
            "Training loss Total loss: 2.1716253757476807 Main loss: 2.1626124382019043 Jacob Loss: 0.09012880176305771 [200/782]\n",
            "Training loss Total loss: 2.149374008178711 Main loss: 2.119419574737549 Jacob Loss: 0.2995452880859375 [300/782]\n",
            "Training loss Total loss: 2.149991750717163 Main loss: 2.1078574657440186 Jacob Loss: 0.42134392261505127 [400/782]\n",
            "Training loss Total loss: 2.101565361022949 Main loss: 2.044743299484253 Jacob Loss: 0.5682212710380554 [500/782]\n",
            "Training loss Total loss: 2.149871587753296 Main loss: 2.0887625217437744 Jacob Loss: 0.611090898513794 [600/782]\n",
            "Training loss Total loss: 2.0146830081939697 Main loss: 1.9444307088851929 Jacob Loss: 0.7025219202041626 [700/782]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loss Total loss: 2.131319761276245 Main loss: 2.070870876312256 Jacob Loss: 0.6044883728027344 [0/782]\n",
            "Training loss Total loss: 2.053279399871826 Main loss: 1.9833765029907227 Jacob Loss: 0.6990287899971008 [100/782]\n",
            "Training loss Total loss: 1.9583828449249268 Main loss: 1.8865169286727905 Jacob Loss: 0.7186589241027832 [200/782]\n",
            "Training loss Total loss: 2.1109416484832764 Main loss: 2.039796829223633 Jacob Loss: 0.7114490270614624 [300/782]\n",
            "Training loss Total loss: 2.068927764892578 Main loss: 1.9946104288101196 Jacob Loss: 0.7431723475456238 [400/782]\n",
            "Training loss Total loss: 2.0112411975860596 Main loss: 1.9353647232055664 Jacob Loss: 0.758765459060669 [500/782]\n",
            "Training loss Total loss: 2.122624397277832 Main loss: 2.045448064804077 Jacob Loss: 0.7717640399932861 [600/782]\n",
            "Training loss Total loss: 1.936075210571289 Main loss: 1.8644514083862305 Jacob Loss: 0.7162376642227173 [700/782]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loss Total loss: 2.0671002864837646 Main loss: 1.9974215030670166 Jacob Loss: 0.6967887282371521 [0/782]\n",
            "Training loss Total loss: 2.0131144523620605 Main loss: 1.9380508661270142 Jacob Loss: 0.7506357431411743 [100/782]\n",
            "Training loss Total loss: 1.8799219131469727 Main loss: 1.8045599460601807 Jacob Loss: 0.7536200881004333 [200/782]\n",
            "Training loss Total loss: 2.068260908126831 Main loss: 1.9909207820892334 Jacob Loss: 0.7734023332595825 [300/782]\n",
            "Training loss Total loss: 2.0220131874084473 Main loss: 1.9376038312911987 Jacob Loss: 0.844092845916748 [400/782]\n",
            "Training loss Total loss: 1.9451582431793213 Main loss: 1.8798521757125854 Jacob Loss: 0.6530603170394897 [500/782]\n",
            "Training loss Total loss: 2.1036465167999268 Main loss: 2.0148568153381348 Jacob Loss: 0.8878976106643677 [600/782]\n",
            "Training loss Total loss: 1.8864467144012451 Main loss: 1.8133403062820435 Jacob Loss: 0.731063723564148 [700/782]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loss Total loss: 2.035839796066284 Main loss: 1.948990821838379 Jacob Loss: 0.8684890270233154 [0/782]\n",
            "Training loss Total loss: 1.9795671701431274 Main loss: 1.8970128297805786 Jacob Loss: 0.8255430459976196 [100/782]\n",
            "Training loss Total loss: 1.8196614980697632 Main loss: 1.736510992050171 Jacob Loss: 0.8315050005912781 [200/782]\n",
            "Training loss Total loss: 2.031076431274414 Main loss: 1.9441713094711304 Jacob Loss: 0.8690518140792847 [300/782]\n",
            "Training loss Total loss: 1.9747159481048584 Main loss: 1.8877644538879395 Jacob Loss: 0.8695151209831238 [400/782]\n",
            "Training loss Total loss: 1.942924976348877 Main loss: 1.8475627899169922 Jacob Loss: 0.9536212682723999 [500/782]\n",
            "Training loss Total loss: 2.07808518409729 Main loss: 1.9880807399749756 Jacob Loss: 0.9000441431999207 [600/782]\n",
            "Training loss Total loss: 1.8618956804275513 Main loss: 1.775221347808838 Jacob Loss: 0.8667429089546204 [700/782]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loss Total loss: 2.0128860473632812 Main loss: 1.9142144918441772 Jacob Loss: 0.9867150783538818 [0/782]\n",
            "Training loss Total loss: 1.965486764907837 Main loss: 1.866443157196045 Jacob Loss: 0.9904364347457886 [100/782]\n",
            "Training loss Total loss: 1.7729469537734985 Main loss: 1.6844323873519897 Jacob Loss: 0.885145902633667 [200/782]\n",
            "Training loss Total loss: 1.99652898311615 Main loss: 1.9032841920852661 Jacob Loss: 0.9324483871459961 [300/782]\n",
            "Training loss Total loss: 1.943407416343689 Main loss: 1.844549298286438 Jacob Loss: 0.9885807037353516 [400/782]\n",
            "Training loss Total loss: 1.9160274267196655 Main loss: 1.8188512325286865 Jacob Loss: 0.9717614650726318 [500/782]\n",
            "Training loss Total loss: 2.062119245529175 Main loss: 1.9571131467819214 Jacob Loss: 1.0500612258911133 [600/782]\n",
            "Training loss Total loss: 1.834944725036621 Main loss: 1.7450153827667236 Jacob Loss: 0.8992931842803955 [700/782]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.9891748428344727 Main loss: 1.8870099782943726 Jacob Loss: 1.0216484069824219 [0/782]\n",
            "Training loss Total loss: 1.9410775899887085 Main loss: 1.84295654296875 Jacob Loss: 0.981210470199585 [100/782]\n",
            "Training loss Total loss: 1.7302066087722778 Main loss: 1.643136739730835 Jacob Loss: 0.8706983327865601 [200/782]\n",
            "Training loss Total loss: 1.9660271406173706 Main loss: 1.8682670593261719 Jacob Loss: 0.9776010513305664 [300/782]\n",
            "Training loss Total loss: 1.9107818603515625 Main loss: 1.8095914125442505 Jacob Loss: 1.0119045972824097 [400/782]\n",
            "Training loss Total loss: 1.8952380418777466 Main loss: 1.7966173887252808 Jacob Loss: 0.9862070679664612 [500/782]\n",
            "Training loss Total loss: 2.0337860584259033 Main loss: 1.9273512363433838 Jacob Loss: 1.0643489360809326 [600/782]\n",
            "Training loss Total loss: 1.8228636980056763 Main loss: 1.7207062244415283 Jacob Loss: 1.0215752124786377 [700/782]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.9570478200912476 Main loss: 1.8630322217941284 Jacob Loss: 0.9401561617851257 [0/782]\n",
            "Training loss Total loss: 1.92019522190094 Main loss: 1.8233811855316162 Jacob Loss: 0.9681401252746582 [100/782]\n",
            "Training loss Total loss: 1.7173810005187988 Main loss: 1.6084378957748413 Jacob Loss: 1.089431643486023 [200/782]\n",
            "Training loss Total loss: 1.937411904335022 Main loss: 1.839935541152954 Jacob Loss: 0.9747641086578369 [300/782]\n",
            "Training loss Total loss: 1.8834353685379028 Main loss: 1.780529499053955 Jacob Loss: 1.0290589332580566 [400/782]\n",
            "Training loss Total loss: 1.889595627784729 Main loss: 1.7781908512115479 Jacob Loss: 1.1140482425689697 [500/782]\n",
            "Training loss Total loss: 2.0070600509643555 Main loss: 1.8987236022949219 Jacob Loss: 1.0833656787872314 [600/782]\n",
            "Training loss Total loss: 1.8115547895431519 Main loss: 1.7000097036361694 Jacob Loss: 1.115450382232666 [700/782]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.9433263540267944 Main loss: 1.8418985605239868 Jacob Loss: 1.014277458190918 [0/782]\n",
            "Training loss Total loss: 1.9213078022003174 Main loss: 1.8070896863937378 Jacob Loss: 1.1421809196472168 [100/782]\n",
            "Training loss Total loss: 1.6853879690170288 Main loss: 1.5775146484375 Jacob Loss: 1.0787328481674194 [200/782]\n",
            "Training loss Total loss: 1.9277750253677368 Main loss: 1.8181313276290894 Jacob Loss: 1.096436619758606 [300/782]\n",
            "Training loss Total loss: 1.8599789142608643 Main loss: 1.7525501251220703 Jacob Loss: 1.0742878913879395 [400/782]\n",
            "Training loss Total loss: 1.8695180416107178 Main loss: 1.7614173889160156 Jacob Loss: 1.0810070037841797 [500/782]\n",
            "Training loss Total loss: 1.9793596267700195 Main loss: 1.8726826906204224 Jacob Loss: 1.0667698383331299 [600/782]\n",
            "Training loss Total loss: 1.8007609844207764 Main loss: 1.6839154958724976 Jacob Loss: 1.168454647064209 [700/782]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.939378023147583 Main loss: 1.8232722282409668 Jacob Loss: 1.161057472229004 [0/782]\n",
            "Training loss Total loss: 1.9171202182769775 Main loss: 1.7937785387039185 Jacob Loss: 1.2334165573120117 [100/782]\n",
            "Training loss Total loss: 1.669955849647522 Main loss: 1.5486047267913818 Jacob Loss: 1.2135114669799805 [200/782]\n",
            "Training loss Total loss: 1.925174355506897 Main loss: 1.8007426261901855 Jacob Loss: 1.2443175315856934 [300/782]\n",
            "Training loss Total loss: 1.8492449522018433 Main loss: 1.7316582202911377 Jacob Loss: 1.1758675575256348 [400/782]\n",
            "Training loss Total loss: 1.880650520324707 Main loss: 1.7514768838882446 Jacob Loss: 1.291736125946045 [500/782]\n",
            "Training loss Total loss: 1.971192479133606 Main loss: 1.851979374885559 Jacob Loss: 1.1921310424804688 [600/782]\n",
            "Training loss Total loss: 1.7901380062103271 Main loss: 1.675048828125 Jacob Loss: 1.1508920192718506 [700/782]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.9198473691940308 Main loss: 1.8064478635787964 Jacob Loss: 1.1339948177337646 [0/782]\n",
            "Training loss Total loss: 1.8941256999969482 Main loss: 1.7800863981246948 Jacob Loss: 1.1403932571411133 [100/782]\n",
            "Training loss Total loss: 1.6523900032043457 Main loss: 1.5302162170410156 Jacob Loss: 1.2217373847961426 [200/782]\n",
            "Training loss Total loss: 1.9169650077819824 Main loss: 1.7873479127883911 Jacob Loss: 1.2961704730987549 [300/782]\n",
            "Training loss Total loss: 1.8393765687942505 Main loss: 1.7125378847122192 Jacob Loss: 1.2683870792388916 [400/782]\n",
            "Training loss Total loss: 1.8667532205581665 Main loss: 1.7443556785583496 Jacob Loss: 1.2239751815795898 [500/782]\n",
            "Training loss Total loss: 1.9488041400909424 Main loss: 1.8368370532989502 Jacob Loss: 1.1196706295013428 [600/782]\n",
            "Training loss Total loss: 1.7831982374191284 Main loss: 1.6669613122940063 Jacob Loss: 1.1623692512512207 [700/782]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.924904465675354 Main loss: 1.791990876197815 Jacob Loss: 1.3291356563568115 [0/782]\n",
            "Training loss Total loss: 1.8798768520355225 Main loss: 1.7663899660110474 Jacob Loss: 1.134868860244751 [100/782]\n",
            "Training loss Total loss: 1.6306720972061157 Main loss: 1.5128625631332397 Jacob Loss: 1.1780953407287598 [200/782]\n",
            "Training loss Total loss: 1.9053195714950562 Main loss: 1.7757506370544434 Jacob Loss: 1.2956898212432861 [300/782]\n",
            "Training loss Total loss: 1.8150043487548828 Main loss: 1.6966474056243896 Jacob Loss: 1.1835697889328003 [400/782]\n",
            "Training loss Total loss: 1.8514776229858398 Main loss: 1.7353712320327759 Jacob Loss: 1.1610643863677979 [500/782]\n",
            "Training loss Total loss: 1.942084789276123 Main loss: 1.825051188468933 Jacob Loss: 1.1703360080718994 [600/782]\n",
            "Training loss Total loss: 1.7899190187454224 Main loss: 1.6607506275177002 Jacob Loss: 1.2916841506958008 [700/782]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.905868649482727 Main loss: 1.7757275104522705 Jacob Loss: 1.3014111518859863 [0/782]\n",
            "Training loss Total loss: 1.8668832778930664 Main loss: 1.7540918588638306 Jacob Loss: 1.1279141902923584 [100/782]\n",
            "Training loss Total loss: 1.633130431175232 Main loss: 1.5031300783157349 Jacob Loss: 1.3000037670135498 [200/782]\n",
            "Training loss Total loss: 1.8947083950042725 Main loss: 1.7698346376419067 Jacob Loss: 1.2487378120422363 [300/782]\n",
            "Training loss Total loss: 1.8076354265213013 Main loss: 1.6830848455429077 Jacob Loss: 1.2455055713653564 [400/782]\n",
            "Training loss Total loss: 1.859090805053711 Main loss: 1.727657675743103 Jacob Loss: 1.3143316507339478 [500/782]\n",
            "Training loss Total loss: 1.9310967922210693 Main loss: 1.812166690826416 Jacob Loss: 1.189300537109375 [600/782]\n",
            "Training loss Total loss: 1.7741973400115967 Main loss: 1.6552884578704834 Jacob Loss: 1.1890888214111328 [700/782]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.887702226638794 Main loss: 1.7614233493804932 Jacob Loss: 1.2627885341644287 [0/782]\n",
            "Training loss Total loss: 1.8571405410766602 Main loss: 1.7429381608963013 Jacob Loss: 1.142024278640747 [100/782]\n",
            "Training loss Total loss: 1.6073188781738281 Main loss: 1.4884240627288818 Jacob Loss: 1.188948631286621 [200/782]\n",
            "Training loss Total loss: 1.8875102996826172 Main loss: 1.7648392915725708 Jacob Loss: 1.226710557937622 [300/782]\n",
            "Training loss Total loss: 1.7950422763824463 Main loss: 1.6733720302581787 Jacob Loss: 1.2167026996612549 [400/782]\n",
            "Training loss Total loss: 1.8420379161834717 Main loss: 1.7216905355453491 Jacob Loss: 1.203473687171936 [500/782]\n",
            "Training loss Total loss: 1.9294602870941162 Main loss: 1.800784707069397 Jacob Loss: 1.2867558002471924 [600/782]\n",
            "Training loss Total loss: 1.7808258533477783 Main loss: 1.6517213582992554 Jacob Loss: 1.2910443544387817 [700/782]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.8742482662200928 Main loss: 1.749009132385254 Jacob Loss: 1.2523913383483887 [0/782]\n",
            "Training loss Total loss: 1.8642089366912842 Main loss: 1.7340513467788696 Jacob Loss: 1.3015754222869873 [100/782]\n",
            "Training loss Total loss: 1.6049716472625732 Main loss: 1.4763805866241455 Jacob Loss: 1.2859103679656982 [200/782]\n",
            "Training loss Total loss: 1.8830431699752808 Main loss: 1.7599292993545532 Jacob Loss: 1.2311384677886963 [300/782]\n",
            "Training loss Total loss: 1.7961357831954956 Main loss: 1.66648268699646 Jacob Loss: 1.2965307235717773 [400/782]\n",
            "Training loss Total loss: 1.8516247272491455 Main loss: 1.7177163362503052 Jacob Loss: 1.3390834331512451 [500/782]\n",
            "Training loss Total loss: 1.905104637145996 Main loss: 1.7903308868408203 Jacob Loss: 1.1477371454238892 [600/782]\n",
            "Training loss Total loss: 1.7712101936340332 Main loss: 1.647298812866211 Jacob Loss: 1.2391135692596436 [700/782]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.8668335676193237 Main loss: 1.7383220195770264 Jacob Loss: 1.2851159572601318 [0/782]\n",
            "Training loss Total loss: 1.8473763465881348 Main loss: 1.7241237163543701 Jacob Loss: 1.2325266599655151 [100/782]\n",
            "Training loss Total loss: 1.5994657278060913 Main loss: 1.4665554761886597 Jacob Loss: 1.3291022777557373 [200/782]\n",
            "Training loss Total loss: 1.8710094690322876 Main loss: 1.7550327777862549 Jacob Loss: 1.159766435623169 [300/782]\n",
            "Training loss Total loss: 1.7906213998794556 Main loss: 1.65835440158844 Jacob Loss: 1.322670340538025 [400/782]\n",
            "Training loss Total loss: 1.8397188186645508 Main loss: 1.711693286895752 Jacob Loss: 1.28025484085083 [500/782]\n",
            "Training loss Total loss: 1.920702576637268 Main loss: 1.7811185121536255 Jacob Loss: 1.3958410024642944 [600/782]\n",
            "Training loss Total loss: 1.7779465913772583 Main loss: 1.6427316665649414 Jacob Loss: 1.352149248123169 [700/782]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.8477756977081299 Main loss: 1.7295364141464233 Jacob Loss: 1.1823925971984863 [0/782]\n",
            "Training loss Total loss: 1.8525068759918213 Main loss: 1.716762900352478 Jacob Loss: 1.3574399948120117 [100/782]\n",
            "Training loss Total loss: 1.5852051973342896 Main loss: 1.45669424533844 Jacob Loss: 1.285109519958496 [200/782]\n",
            "Training loss Total loss: 1.891879677772522 Main loss: 1.7506365776062012 Jacob Loss: 1.4124313592910767 [300/782]\n",
            "Training loss Total loss: 1.7812925577163696 Main loss: 1.6525899171829224 Jacob Loss: 1.2870264053344727 [400/782]\n",
            "Training loss Total loss: 1.8323934078216553 Main loss: 1.7067791223526 Jacob Loss: 1.2561428546905518 [500/782]\n",
            "Training loss Total loss: 1.89968740940094 Main loss: 1.7740962505340576 Jacob Loss: 1.2559117078781128 [600/782]\n",
            "Training loss Total loss: 1.7798442840576172 Main loss: 1.6398905515670776 Jacob Loss: 1.399537444114685 [700/782]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.8497213125228882 Main loss: 1.7189067602157593 Jacob Loss: 1.3081457614898682 [0/782]\n",
            "Training loss Total loss: 1.8375353813171387 Main loss: 1.706502079963684 Jacob Loss: 1.310333490371704 [100/782]\n",
            "Training loss Total loss: 1.5744138956069946 Main loss: 1.4474302530288696 Jacob Loss: 1.2698369026184082 [200/782]\n",
            "Training loss Total loss: 1.8916946649551392 Main loss: 1.7465702295303345 Jacob Loss: 1.451244831085205 [300/782]\n",
            "Training loss Total loss: 1.7807823419570923 Main loss: 1.6481486558914185 Jacob Loss: 1.32633638381958 [400/782]\n",
            "Training loss Total loss: 1.8285176753997803 Main loss: 1.704003930091858 Jacob Loss: 1.2451369762420654 [500/782]\n",
            "Training loss Total loss: 1.896777629852295 Main loss: 1.769097924232483 Jacob Loss: 1.2767971754074097 [600/782]\n",
            "Training loss Total loss: 1.7648475170135498 Main loss: 1.637176752090454 Jacob Loss: 1.2767078876495361 [700/782]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.8364609479904175 Main loss: 1.710039734840393 Jacob Loss: 1.264211654663086 [0/782]\n",
            "Training loss Total loss: 1.8382368087768555 Main loss: 1.6980985403060913 Jacob Loss: 1.4013829231262207 [100/782]\n",
            "Training loss Total loss: 1.5727931261062622 Main loss: 1.4402549266815186 Jacob Loss: 1.3253819942474365 [200/782]\n",
            "Training loss Total loss: 1.876964807510376 Main loss: 1.744180679321289 Jacob Loss: 1.3278417587280273 [300/782]\n",
            "Training loss Total loss: 1.7796261310577393 Main loss: 1.6441080570220947 Jacob Loss: 1.3551812171936035 [400/782]\n",
            "Training loss Total loss: 1.8377982378005981 Main loss: 1.7022993564605713 Jacob Loss: 1.3549888134002686 [500/782]\n",
            "Training loss Total loss: 1.9007130861282349 Main loss: 1.7642076015472412 Jacob Loss: 1.3650544881820679 [600/782]\n",
            "Training loss Total loss: 1.7653110027313232 Main loss: 1.6343398094177246 Jacob Loss: 1.3097116947174072 [700/782]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.8344887495040894 Main loss: 1.7021782398223877 Jacob Loss: 1.3231055736541748 [0/782]\n",
            "Training loss Total loss: 1.8231706619262695 Main loss: 1.6923543214797974 Jacob Loss: 1.3081634044647217 [100/782]\n",
            "Training loss Total loss: 1.5690909624099731 Main loss: 1.434621810913086 Jacob Loss: 1.344691276550293 [200/782]\n",
            "Training loss Total loss: 1.8621501922607422 Main loss: 1.742172360420227 Jacob Loss: 1.1997785568237305 [300/782]\n",
            "Training loss Total loss: 1.7683848142623901 Main loss: 1.639042854309082 Jacob Loss: 1.293419361114502 [400/782]\n",
            "Training loss Total loss: 1.8391501903533936 Main loss: 1.699576497077942 Jacob Loss: 1.3957363367080688 [500/782]\n",
            "Training loss Total loss: 1.9035849571228027 Main loss: 1.7600387334823608 Jacob Loss: 1.435462474822998 [600/782]\n",
            "Training loss Total loss: 1.7668169736862183 Main loss: 1.6327120065689087 Jacob Loss: 1.3410500288009644 [700/782]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loss Total loss: 1.819970965385437 Main loss: 1.6956782341003418 Jacob Loss: 1.242926836013794 [0/782]\n",
            "Training loss Total loss: 1.8175357580184937 Main loss: 1.6846163272857666 Jacob Loss: 1.3291940689086914 [100/782]\n",
            "Training loss Total loss: 1.5576715469360352 Main loss: 1.4274550676345825 Jacob Loss: 1.3021646738052368 [200/782]\n",
            "Training loss Total loss: 1.8610376119613647 Main loss: 1.7368830442428589 Jacob Loss: 1.2415452003479004 [300/782]\n",
            "Training loss Total loss: 1.7670955657958984 Main loss: 1.6328320503234863 Jacob Loss: 1.3426356315612793 [400/782]\n",
            "Training loss Total loss: 1.8404213190078735 Main loss: 1.6942487955093384 Jacob Loss: 1.4617257118225098 [500/782]\n",
            "Training loss Total loss: 1.8830227851867676 Main loss: 1.7546405792236328 Jacob Loss: 1.2838218212127686 [600/782]\n",
            "Training loss Total loss: 1.7828410863876343 Main loss: 1.6324712038040161 Jacob Loss: 1.503698468208313 [700/782]\n",
            "Done!\n",
            "Test Error: \n",
            " Accuracy: 44.98%, Avg loss: 138.15072631835938, Main loss: 1.612110999739094, Jacob loss: 1.3574118614196777 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U1w7NhomDrF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}